{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "fba5bafe",
   "metadata": {},
   "source": [
    "### Fine-tuning un LLM grâce à LoRA\n",
    "\n",
    "#### Rappels théoriques: \n",
    "\n",
    "TODO: LoRA\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "#### PEFT - HuggingFace:\n",
    "\n",
    "PEFT (Parameter-Efficient Fine-Tuning) is a library for efficiently adapting large pretrained models to various downstream applications without fine-tuning all of a model’s parameters because it is prohibitively costly. PEFT methods only fine-tune a small number of (extra) model parameters - significantly decreasing computational and storage costs - while yielding performance comparable to a fully fine-tuned model. This makes it more accessible to train and store large language models (LLMs) on consumer hardware.\n",
    "\n",
    "#### Workflow:\n",
    "\n",
    "Loading the model + dataset -> Define LoRA config -> Create PEFT model -> Training -> Save the model (pushing to HF) -> Quantization (optional) -> Inference\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "b3f0455d",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\u001b[34m\u001b[1mwandb\u001b[0m: \u001b[33mWARNING\u001b[0m If you're specifying your api key in code, ensure this code is not shared publicly.\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m: \u001b[33mWARNING\u001b[0m Consider setting the WANDB_API_KEY environment variable, or running `wandb login` from the command line.\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m: Appending key for api.wandb.ai to your netrc file: /home/brikiyou/.netrc\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "True"
      ]
     },
     "execution_count": 3,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# Wandb login\n",
    "\n",
    "import wandb\n",
    "from dotenv import load_dotenv, find_dotenv\n",
    "import os\n",
    "import getpass\n",
    "\n",
    "load_dotenv(find_dotenv())\n",
    "\n",
    "def get_api_key(env_var, prompt):\n",
    "    if not os.getenv(env_var):\n",
    "        os.environ[env_var] = getpass.getpass(prompt)\n",
    "\n",
    "get_api_key(\"WANDB_API_KEY\", \"Enter your Weights & Biases API key: \")\n",
    "wandb.login(key=os.getenv(\"WANDB_API_KEY\"))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "fb612321",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/scratch/brikiyou/ift6289/IFT6285-fine-tuning-demo/.venv/lib/python3.10/site-packages/transformers/utils/hub.py:110: FutureWarning: Using `TRANSFORMERS_CACHE` is deprecated and will be removed in v5 of Transformers. Use `HF_HOME` instead.\n",
      "  warnings.warn(\n"
     ]
    }
   ],
   "source": [
    "from peft import LoraConfig, TaskType, get_peft_model\n",
    "from transformers import AutoTokenizer, Qwen3ForSequenceClassification, default_data_collator, EarlyStoppingCallback, TrainingArguments, Trainer\n",
    "import torch\n",
    "import os \n",
    "import pandas as pd\n",
    "from datasets import Dataset, DatasetDict\n",
    "\n",
    "DEVICE = torch.device(\"cuda\" if torch.cuda.is_available() else \"cpu\")\n",
    "NUM_GPUS = torch.cuda.device_count()\n",
    "BF16 = torch.cuda.is_bf16_supported()\n",
    "\n",
    "os.environ[\"HF_HOME\"] =  os.path.join(os.environ[\"SCRATCH\"], \"huggingface_cache\")\n",
    "os.environ[\"HF_HUB_CACHE\"]       = os.path.join(os.environ[\"HF_HOME\"], \"hub\")\n",
    "os.environ[\"TRANSFORMERS_CACHE\"] = os.path.join(os.environ[\"HF_HOME\"], \"models\")\n",
    "os.environ[\"HF_DATASETS_CACHE\"]  = os.path.join(os.environ[\"HF_HOME\"], \"datasets\")\n",
    "\n",
    "\n",
    "cache_dir = os.environ[\"HF_HOME\"]\n",
    "\n",
    "# TODO: To change the following stuff \n",
    "data_dir = os.path.join(os.getcwd(), ) \n",
    "out_dir = os.path.join(os.getcwd(), \"\") \n",
    "\n",
    "os.mkdir(out_dir, exist_ok=True)\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "10455c56",
   "metadata": {},
   "source": [
    "Loading the model and tokenizer:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "2b7825de",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "b60e0f8f1ebd42d2abdc8cb0091e72ab",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Loading checkpoint shards:   0%|          | 0/2 [00:00<?, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Some weights of Qwen3ForSequenceClassification were not initialized from the model checkpoint at Qwen/Qwen3-1.7B and are newly initialized: ['score.weight']\n",
      "You should probably TRAIN this model on a down-stream task to be able to use it for predictions and inference.\n"
     ]
    }
   ],
   "source": [
    "model_name = \"Qwen/Qwen3-1.7B\"\n",
    "model = Qwen3ForSequenceClassification.from_pretrained(model_name, \n",
    "                                                       dtype=torch.float32,\n",
    "                                                       device_map=\"auto\",\n",
    "                                                       num_labels=3,\n",
    "                                                       trust_remote_code=True,\n",
    "                                                       cache_dir=cache_dir)\n",
    "tokenizer = AutoTokenizer.from_pretrained(model_name,\n",
    "                                         cache_dir=cache_dir)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "2220fd9b",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Qwen3ForSequenceClassification(\n",
      "  (model): Qwen3Model(\n",
      "    (embed_tokens): Embedding(151936, 2048)\n",
      "    (layers): ModuleList(\n",
      "      (0-27): 28 x Qwen3DecoderLayer(\n",
      "        (self_attn): Qwen3Attention(\n",
      "          (q_proj): Linear(in_features=2048, out_features=2048, bias=False)\n",
      "          (k_proj): Linear(in_features=2048, out_features=1024, bias=False)\n",
      "          (v_proj): Linear(in_features=2048, out_features=1024, bias=False)\n",
      "          (o_proj): Linear(in_features=2048, out_features=2048, bias=False)\n",
      "          (q_norm): Qwen3RMSNorm((128,), eps=1e-06)\n",
      "          (k_norm): Qwen3RMSNorm((128,), eps=1e-06)\n",
      "        )\n",
      "        (mlp): Qwen3MLP(\n",
      "          (gate_proj): Linear(in_features=2048, out_features=6144, bias=False)\n",
      "          (up_proj): Linear(in_features=2048, out_features=6144, bias=False)\n",
      "          (down_proj): Linear(in_features=6144, out_features=2048, bias=False)\n",
      "          (act_fn): SiLUActivation()\n",
      "        )\n",
      "        (input_layernorm): Qwen3RMSNorm((2048,), eps=1e-06)\n",
      "        (post_attention_layernorm): Qwen3RMSNorm((2048,), eps=1e-06)\n",
      "      )\n",
      "    )\n",
      "    (norm): Qwen3RMSNorm((2048,), eps=1e-06)\n",
      "    (rotary_emb): Qwen3RotaryEmbedding()\n",
      "  )\n",
      "  (score): Linear(in_features=2048, out_features=3, bias=False)\n",
      ")\n"
     ]
    }
   ],
   "source": [
    "print(model)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "ab1a747b-31ad-4178-bc7c-3df4189ac976",
   "metadata": {},
   "source": [
    "### Quoi modifier ici \n",
    "Tous les élements de self attention mechanisme + mlp à part act_fn\n",
    "\n",
    "Maintenant nous devons utiliser LoraConfig"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "id": "d636a0a4-dcff-42f1-b94c-171b91e0dbd8",
   "metadata": {},
   "outputs": [],
   "source": [
    "peft_config = LoraConfig(\n",
    "    r=8,\n",
    "    lora_alpha=32,\n",
    "    target_modules=[\n",
    "        \"q_proj\",\"v_proj\",\"k_proj\",\"o_proj\",\n",
    "        \"gate_proj\",\"up_proj\",\"down_proj\",\n",
    "        #\"lora_magnitude_vector\"  # if using DoRa\n",
    "    ],\n",
    "    lora_dropout=0.01,\n",
    "    bias=\"none\",\n",
    "    task_type=TaskType.SEQ_CLS,\n",
    "    #use_dora=True,\n",
    ")\n",
    "\n",
    "lora_model = get_peft_model(model, peft_config)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "2190fa96-3b14-4e0d-8554-c6375462f9f6",
   "metadata": {},
   "source": [
    "## Loading Data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "31002e4f-299f-4f8a-b209-8f57359f5ad9",
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "def get_training_files(data_aug:bool=False, *args):\n",
    "    \"\"\"\n",
    "    Returns training files we're going to use\n",
    "\n",
    "    Args:\n",
    "        data_aug (bool, optional): _description_. Defaults to False.\n",
    "\n",
    "    Returns:\n",
    "        _type_: _description_\n",
    "    \"\"\"\n",
    "    if data_aug:\n",
    "        return [\n",
    "            \"Sentences_75Agree_utf8.txt\",\n",
    "            \"Sentences_AllAgree_utf8.txt\",\n",
    "            \"Augmented_Sentences_utf8.txt\",\n",
    "            args[:] # To change here \n",
    "        ]\n",
    "    else:\n",
    "         return [\n",
    "            \"Sentences_75Agree_utf8.txt\",\n",
    "            \"Sentences_AllAgree_utf8.txt\",\n",
    "            args[:]\n",
    "        ]\n",
    "         \n",
    "         \n",
    "training_files = get_training_files(data_aug=False)\n",
    "test_file = \"Sentences_50Agree_utf8.txt\""
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": ".venv",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.13"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
