{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "991be941",
   "metadata": {},
   "source": [
    "# AI agents with LangChain"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "id": "3eec7b23",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "True"
      ]
     },
     "execution_count": 12,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "import os\n",
    "from dotenv import load_dotenv, find_dotenv\n",
    "import os\n",
    "import getpass\n",
    "from typing import Annotated, Any, Dict, List, Optional\n",
    "from dataclasses import dataclass, field\n",
    "\n",
    "# Imports LangChain / LangGraph\n",
    "from langchain_openai import ChatOpenAI\n",
    "from langchain_core.tools import tool\n",
    "from langchain_core.messages import HumanMessage, AIMessage\n",
    "from langchain_core.prompts import ChatPromptTemplate\n",
    "from langchain_core.runnables import RunnableConfig\n",
    "from langchain_core.vectorstores.in_memory import InMemoryVectorStore\n",
    "from langchain_openai.embeddings import OpenAIEmbeddings\n",
    "from langchain_core.documents import Document\n",
    "from langgraph.prebuilt import create_react_agent, InjectedState\n",
    "from langgraph.graph import MessagesState, StateGraph, START, END\n",
    "from langgraph.types import Command, Send\n",
    "from langgraph.checkpoint.memory import MemorySaver\n",
    "from pydantic import BaseModel, ConfigDict, Field, PrivateAttr\n",
    "from dotenv import load_dotenv\n",
    "import asyncio\n",
    "import uuid\n",
    "\n",
    "\n",
    "load_dotenv(find_dotenv())"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "600532a5",
   "metadata": {},
   "source": [
    "### Agents sur langchain avec create_react_agent"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "id": "7561b05f",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "--- D√©marrage de l'Agent Moderne (LangGraph) ---\n",
      "La longueur du mot 'Anticonstitutionnellement' multipli√©e par 5 est 125.\n",
      "\n",
      "--- Historique des interactions ---\n",
      "[HUMAN]: Quelle est la longueur du mot 'Anticonstitutionnellement' multipli√©e par 5 ?\n",
      "[AI]: \n",
      "[TOOL]: 25\n",
      "[AI]: \n",
      "[TOOL]: 125\n",
      "[AI]: La longueur du mot 'Anticonstitutionnellement' multipli√©e par 5 est 125.\n"
     ]
    }
   ],
   "source": [
    "import os\n",
    "from langchain_openai import ChatOpenAI\n",
    "from langchain_core.tools import tool\n",
    "from langchain.agents import create_agent\n",
    "\n",
    "@tool\n",
    "def get_word_length(word: str) -> int:\n",
    "    \"\"\"Retourne la longueur (nombre de lettres) d'un mot donn√©.\"\"\"\n",
    "    return len(word)\n",
    "\n",
    "@tool\n",
    "def magic_multiplier(a: int, b: int) -> int:\n",
    "    \"\"\"Multiplie deux nombres d'une mani√®re magique.\"\"\"\n",
    "    return a * b\n",
    "\n",
    "tools = [get_word_length, magic_multiplier]\n",
    "model = ChatOpenAI(model=\"gpt-4o-mini\", temperature=0)\n",
    "\n",
    "agent_graph = create_agent(model, tools)\n",
    "\n",
    "print(\"--- D√©marrage de l'Agent Moderne (LangGraph) ---\")\n",
    "\n",
    "query = \"Quelle est la longueur du mot 'Anticonstitutionnellement' multipli√©e par 5 ?\"\n",
    "\n",
    "inputs = {\"messages\": [(\"user\", query)]}\n",
    "\n",
    "result = agent_graph.invoke(inputs)\n",
    "\n",
    "last_message = result[\"messages\"][-1]\n",
    "print(last_message.content)\n",
    "\n",
    "print(\"\\n--- Historique des interactions ---\")\n",
    "for msg in result[\"messages\"]:\n",
    "    print(f\"[{msg.type.upper()}]: {msg.content}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "b9c4ffcf",
   "metadata": {},
   "source": [
    "### Multi-agentic systems with LangGraph - "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "id": "ab657700",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Graph saved to high_level_supervisor.png\n",
      "--- TEST 1: MATH ---\n",
      "\n",
      "User Input: Calculate 50 + 50\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/tmp/ipykernel_2977499/1156794806.py:77: LangGraphDeprecatedSinceV10: create_react_agent has been moved to `langchain.agents`. Please update your import to `from langchain.agents import create_agent`. Deprecated in LangGraph V1.0 to be removed in V2.0.\n",
      "  joke_agent = create_react_agent(\n",
      "/tmp/ipykernel_2977499/1156794806.py:84: LangGraphDeprecatedSinceV10: create_react_agent has been moved to `langchain.agents`. Please update your import to `from langchain.agents import create_agent`. Deprecated in LangGraph V1.0 to be removed in V2.0.\n",
      "  math_agent = create_react_agent(\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "ü§ñ Final Answer: The result of 50 + 50 is 100.\n",
      "\n",
      "--- TEST 2: JOKE ---\n",
      "\n",
      "User Input: Tell me a joke about Python code.\n",
      "ü§ñ Final Answer: I hope you enjoyed the joke! If you have any more requests, feel free to ask.\n"
     ]
    }
   ],
   "source": [
    "import os\n",
    "import getpass\n",
    "import asyncio\n",
    "from typing import List, Any, Optional\n",
    "from dataclasses import dataclass, field\n",
    "import uuid\n",
    "\n",
    "# --- Imports LangChain / LangGraph ---\n",
    "from langchain_openai import ChatOpenAI\n",
    "from langchain_core.tools import tool\n",
    "from langchain_core.runnables import RunnableConfig\n",
    "from langchain_core.vectorstores.in_memory import InMemoryVectorStore\n",
    "from langchain_openai.embeddings import OpenAIEmbeddings\n",
    "from langchain_core.documents import Document\n",
    "\n",
    "# --- Le \"New Stuff\" ---\n",
    "from langgraph_supervisor import create_supervisor\n",
    "from langgraph.prebuilt import create_react_agent\n",
    "\n",
    "from pydantic import BaseModel, ConfigDict, PrivateAttr\n",
    "from dotenv import load_dotenv\n",
    "\n",
    "\n",
    "\n",
    "@dataclass\n",
    "class EmbeddingModelWrapper:\n",
    "    embeddings: OpenAIEmbeddings = field(default_factory=OpenAIEmbeddings)\n",
    "\n",
    "model_wrapper = EmbeddingModelWrapper()\n",
    "recall_vector_store = InMemoryVectorStore(model_wrapper.embeddings)\n",
    "\n",
    "@tool\n",
    "def save_recall_memory(memory: str, config: RunnableConfig) -> str:\n",
    "    \"\"\"Save memory to vectorstore for later semantic retrieval.\"\"\"\n",
    "    document = Document(page_content=memory, id=str(uuid.uuid4()))\n",
    "    recall_vector_store.add_documents([document])\n",
    "    return \"Memory saved.\"\n",
    "\n",
    "@tool\n",
    "def search_recall_memories(query: str, config: RunnableConfig) -> List[str]:\n",
    "    \"\"\"Search for relevant memories.\"\"\"\n",
    "    documents = recall_vector_store.similarity_search(query, k=3)\n",
    "    return [document.page_content for document in documents]\n",
    "\n",
    "@tool\n",
    "def tell_joke(topic: str) -> str:\n",
    "    \"\"\"Tells a joke about a specific topic.\"\"\"\n",
    "    return f\"Why did the {topic} cross the road? To get to the other side! (Ha ha)\"\n",
    "\n",
    "@tool\n",
    "def add_numbers(a: int, b: int) -> int:\n",
    "    \"\"\"Adds two numbers together.\"\"\"\n",
    "    return a + b\n",
    "\n",
    "\n",
    "class HighLevelSupervisorAgent(BaseModel):\n",
    "    model: ChatOpenAI\n",
    "    \n",
    "    # Le graphe compil√© sera stock√© ici\n",
    "    _app: Any = PrivateAttr(default=None)\n",
    "\n",
    "    model_config = ConfigDict(\n",
    "        extra='forbid',\n",
    "        arbitrary_types_allowed=True, \n",
    "    )\n",
    "\n",
    "    def model_post_init(self, __context: Any) -> None:\n",
    "        \"\"\"\n",
    "        Construit l'architecture Supervisor en utilisant la librairie 'langgraph_supervisor'.\n",
    "        \"\"\"\n",
    "\n",
    "        # A. Cr√©ation des Agents Sp√©cialistes (Workers)\n",
    "        # ---------------------------------------------\n",
    "        # On utilise create_react_agent standard.\n",
    "        # Note: On donne un nom explicite √† chaque agent, c'est ce nom que le superviseur utilisera.\n",
    "        \n",
    "        joke_agent = create_react_agent(\n",
    "            model=self.model,\n",
    "            tools=[tell_joke],\n",
    "            name=\"joke_expert\", # Le nom interne de l'agent\n",
    "            prompt=\"You are a funny assistant. Your goal is to tell jokes when asked.\"\n",
    "        )\n",
    "\n",
    "        math_agent = create_react_agent(\n",
    "            model=self.model,\n",
    "            tools=[add_numbers],\n",
    "            name=\"math_expert\",\n",
    "            prompt=\"You are a math assistant. You only do additions.\"\n",
    "        )\n",
    "\n",
    "        # B. Cr√©ation du Superviseur\n",
    "        # ---------------------------------------------------\n",
    "        # 'create_supervisor' fait tout le travail difficile pour nous :\n",
    "        # 1. Il cr√©e automatiquement les outils de handoff pour \"joke_expert\" et \"math_expert\".\n",
    "        # 2. Il g√®re le graphe d'√©tat et le routage.\n",
    "        # 3. Il ajoute un noeud sp√©cial pour que le superviseur puisse r√©pondre directement.\n",
    "        \n",
    "        workflow = create_supervisor(\n",
    "            agents=[joke_agent, math_agent], # Liste des workers\n",
    "            tools=[save_recall_memory, search_recall_memories], # Outils propres au superviseur\n",
    "            model=self.model,\n",
    "            prompt=(\n",
    "                \"You are a supervisor managing two assistants: 'joke_expert' and 'math_expert'. \"\n",
    "                \"Delegate tasks to them. If the task is finished or general, answer yourself.\"\n",
    "            )\n",
    "        )\n",
    "\n",
    "        self._app = workflow.compile()\n",
    "\n",
    "    def get_graph(self):\n",
    "        return self._app\n",
    "\n",
    "    def draw_image(self):\n",
    "        try:\n",
    "            with open(\"high_level_supervisor.png\", \"wb\") as f:\n",
    "                f.write(self._app.get_graph().draw_mermaid_png())\n",
    "            print(\"Graph saved to high_level_supervisor.png\")\n",
    "        except Exception as e:\n",
    "            print(f\"Could not draw graph: {e}\")\n",
    "\n",
    "    async def arun(self, user_input: str):\n",
    "        print(f\"\\nUser Input: {user_input}\")\n",
    "        \n",
    "        initial_state = {\"messages\": [{\"role\": \"user\", \"content\": user_input}]}\n",
    "        \n",
    "        async for chunk in self._app.astream(initial_state, subgraphs=True):\n",
    "            if \"next\" in chunk:\n",
    "                print(f\"üîÑ Routing to: {chunk['next']}\")\n",
    "            elif \"messages\" in chunk:\n",
    "                pass \n",
    "        final_state = await self._app.ainvoke(initial_state)\n",
    "        print(f\"ü§ñ Final Answer: {final_state['messages'][-1].content}\")\n",
    "\n",
    "    def run(self, user_input: str):\n",
    "        \"\"\"Wrapper intelligent pour g√©rer Jupyter vs Terminal\"\"\"\n",
    "        try:\n",
    "            loop = asyncio.get_running_loop()\n",
    "        except RuntimeError:\n",
    "            loop = None\n",
    "\n",
    "        if loop and loop.is_running():\n",
    "            print(\"‚ö†Ô∏è Dans Jupyter, utilisez 'await agent.arun(...)'\")\n",
    "            return self.arun(user_input)\n",
    "        else:\n",
    "            asyncio.run(self.arun(user_input))\n",
    "\n",
    "\n",
    "if __name__ == \"__main__\":\n",
    "    if not os.environ.get(\"OPENAI_API_KEY\"):\n",
    "        print(\"Please set OPENAI_API_KEY\")\n",
    "        # exit(1)\n",
    "\n",
    "    agent = HighLevelSupervisorAgent(\n",
    "        model=ChatOpenAI(model=\"gpt-4o\", temperature=0),\n",
    "    )\n",
    "\n",
    "    agent.draw_image()\n",
    "    \n",
    "    print(\"--- TEST 1: MATH ---\")\n",
    "    if asyncio.get_event_loop().is_running():\n",
    "        await agent.arun(\"Calculate 50 + 50\")\n",
    "    else:\n",
    "        agent.run(\"Calculate 50 + 50\")\n",
    "    \n",
    "    print(\"\\n--- TEST 2: JOKE ---\")\n",
    "    if asyncio.get_event_loop().is_running():\n",
    "        await agent.arun(\"Tell me a joke about Python code.\")"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": ".venv",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.13"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
